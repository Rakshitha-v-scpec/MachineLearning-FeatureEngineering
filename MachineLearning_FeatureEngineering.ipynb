{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Questions**"
      ],
      "metadata": {
        "id": "IAYkRWUxjqjf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a parameter?\n",
        "\n",
        "    In machine learning, a parameter is one of the internal variables that a model learns from data during training. They define how input features are transformed into outputs and are adjusted to minimize error."
      ],
      "metadata": {
        "id": "nCto49CKjtGL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is correlation?\n",
        "What does negative correlation mean?\n",
        "\n",
        "    Correlation:\n",
        "\n",
        "    A statistical measure showing how much two variables move together.\n",
        "\n",
        "    It does not indicate causation—only that there's a relationship\n",
        "\n",
        "\n",
        "   Negative Correlation:\n",
        "\n",
        "   - A negative correlation (or inverse correlation) occurs when one variable increases while the other decreases, and vice versa\n",
        "\n",
        "   - Graphically, this appears as a downward-sloping line when plotted ."
      ],
      "metadata": {
        "id": "e2L_ZDYEkMu_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "    Machine learning is learning the pattern from the data and replicate in the future.\n",
        "\n",
        "    Main components:\n",
        "\n",
        "    1. Data\n",
        "    2. Algorithms\n",
        "    3. Models\n",
        "    4. Features and so on..."
      ],
      "metadata": {
        "id": "m5K2Wb7hk9os"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "    A loss value in machine learning is a numerical indicator of how far off your model’s predictions are from the true labels—essentially, it measures error using a chosen loss function (like MSE for regression or cross-entropy for classification)"
      ],
      "metadata": {
        "id": "lhTrGlsulmfz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are continuous and categorical variables?\n",
        "\n",
        "    Continuous variables: Numeric, infinite precision, allow full mathematical operations.\n",
        "\n",
        "    Categorical variables: Grouped into categories (nominal or ordinal), no numeric meaning between values."
      ],
      "metadata": {
        "id": "468fckbImCUY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "\n",
        "    Categories are:\n",
        "    1. Label / Ordinal Encoding\n",
        "    2. One‑Hot Encoding\n",
        "    3. Binary / Base‑N Encoding\n",
        "    4. Binary Encoding\n",
        "    5. Target Encoding\n",
        "\n",
        "    Choose encoding method based on: variable type (nominal/ordinal), cardinality, and model type.\n",
        "\n",
        "    Always prevent data leakage by fitting encoders on only training data.\n",
        "\n",
        "    For high-dimensional categories, prefer compact methods (binary, hashing, embeddings).\n",
        "\n",
        "    For interpretability, stick with one‑hot or label encoding where feasible.\n"
      ],
      "metadata": {
        "id": "U_BIf1JLmSo1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What do you mean by training and testing a dataset?\n",
        "\n",
        "    Training a dataset:\n",
        "    The training dataset is used to fit the model’s parameters (like weights in a neural network or coefficients in regression)\n",
        "    \n",
        "    During training:\n",
        "    - The learning algorithm processes input–label pairs (supervised learning).\n",
        "\n",
        "    - It adjusts parameters to reduce error via optimization (e.g., gradient descent)\n",
        "    - This is akin to a student studying from textbooks—the model learns patterns in the data.\n",
        "\n",
        "    Testing a dataset: The test dataset contains unseen examples and is never used during training or hyperparameter tuning\n",
        "    \n",
        "    - After training is complete, the model makes predictions on this data. We then compute metrics (accuracy, precision, F1, etc.)\n",
        "    \n",
        "    - The test performance indicates how well the model generalizes to new real-world data—much like an exam after studying\n"
      ],
      "metadata": {
        "id": "UWbvMKp5nLiM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is sklearn.preprocessing?\n",
        "\n",
        "    sklearn.preprocessing is a submodule in scikit-learn providing tools to transform raw data into formats suited for machine learning models (e.g., scaling, encoding, normalization, and more)"
      ],
      "metadata": {
        "id": "wv5bG66oo6o7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is a Test set?\n",
        "\n",
        "    In machine learning, a test set is a subset of the dataset used to evaluate the performance of a trained model on unseen data. It is kept separate from the training and validation sets to ensure an unbiased assessment of the model's ability to generalize to new, real-world data."
      ],
      "metadata": {
        "id": "l4jAxT1LpHNR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?\n",
        "\n",
        "    Splitting data for model fitting and approaching a machine learning problem are foundational steps in building effective models.\n",
        "\n",
        "          from sklearn.model_selection import train_test_split\n",
        "\n",
        "          X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "bkJtE6VHpSyV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        "    Reasons:\n",
        "\n",
        "    1. Understanding the Dataset\n",
        "    2. Identifying Data Quality Issues\n",
        "    3. Feature Engineering and Transformation\n",
        "    4. Selecting the Right Model\n",
        "    5. Visualizing Data Distributions"
      ],
      "metadata": {
        "id": "_nreUkRmp-yz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is correlation?\n",
        "\n",
        "      A statistical measure showing how much two variables move together.\n",
        "\n",
        "      It does not indicate causation—only that there's a relationship"
      ],
      "metadata": {
        "id": "kgcTosWGqbBB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What does negative correlation mean?\n",
        "\n",
        "    Negative Correlation:\n",
        "\n",
        "    A negative correlation (or inverse correlation) occurs when one variable increases while the other decreases, and vice versa\n",
        "\n",
        "    Graphically, this appears as a downward-sloping line when plotted ."
      ],
      "metadata": {
        "id": "mNsOIgiUqkIw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. How can you find correlation between variables in Python?\n",
        "\n",
        "    1. Using Pandas:\n",
        "Pandas provides a straightforward way to compute the correlation matrix for a DataFrame using the .corr() method. This method calculates the Pearson correlation coefficient by default, which measures the linear relationship between variables.\n",
        "\n",
        "    2. Using NumPy:\n",
        "NumPy's np.corrcoef() function computes the Pearson correlation coefficient for two or more variables.\n",
        "\n",
        "    3. Using Seaborn for Visualization:\n",
        "Seaborn, built on top of Matplotlib, provides a high-level interface for drawing attractive and informative statistical graphics. You can visualize the correlation matrix using a heatmap."
      ],
      "metadata": {
        "id": "DTYcbh-7qygv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What is causation? Explain difference between correlation and causation with an example\n",
        "\n",
        "    Causation refers to a direct cause-and-effect relationship between two variables, where a change in one variable directly leads to a change in another. In contrast, correlation indicates a statistical association between two variables, but it does not imply that one causes the other.\n",
        "\n",
        "    Example: Ice Cream Sales and Sunburns:\n",
        "A classic example illustrating the difference is the relationship between ice cream sales and sunburn incidents. Data may show that both ice cream sales and sunburn cases increase during summer months. However, this is a correlation, not causation. The underlying cause is sun exposure, which leads to both higher ice cream consumption and increased risk of sunburn. Therefore, while the two variables are correlated, one does not cause the other"
      ],
      "metadata": {
        "id": "D2Lft1eXrLoH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        "    In machine learning, an optimizer is an algorithm or method used to adjust the weights and biases of a model to minimize the loss function during training. The goal is to find the optimal parameters that lead to the best performance of the model.\n",
        "\n",
        "    1. Stochastic Gradient Descent (SGD)\n",
        "\n",
        "            from tensorflow.keras.optimizers import SGD\n",
        "            optimizer = SGD(learning_rate=0.01)\n",
        "\n",
        "    2. Momentum\n",
        "\n",
        "            from tensorflow.keras.optimizers import SGD\n",
        "            optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
        "\n",
        "    3. Nesterov Accelerated Gradient (NAG)\n",
        "\n",
        "\n",
        "            from tensorflow.keras.optimizers import SGD\n",
        "            optimizer = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
        "\n",
        "\n",
        "\n",
        "  4. Adagrad (Adaptive Gradient Algorithm)\n",
        "\n",
        "                from tensorflow.keras.optimizers import Adagrad\n",
        "                optimizer = Adagrad(learning_rate=0.01)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rLibGIU5rppm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What is sklearn.linear_model ?\n",
        "\n",
        "    The sklearn.linear_model module in scikit-learn provides a suite of linear models for both regression and classification tasks. These models are foundational in machine learning due to their simplicity, interpretability, and efficiency"
      ],
      "metadata": {
        "id": "X_nNA2_Csez4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What does model.fit() do? What arguments must be given?\n",
        "\n",
        "      In scikit-learn, the model.fit() method is used to train a machine learning model on a given dataset. This method adjusts the model's internal parameters to learn patterns from the data, enabling it to make predictions on new, unseen data.\n",
        "\n",
        "      from sklearn.linear_model import LinearRegression\n",
        "\n",
        "        # Sample training data\n",
        "        X_train = [[1], [2], [3], [4], [5]]\n",
        "        y_train = [1, 2, 3, 4, 5]\n",
        "\n",
        "        # Initialize the model\n",
        "        model = LinearRegression()\n",
        "\n",
        "        # Fit the model to the training data\n",
        "        model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "DsAOBRWasoKc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What does model.predict() do? What arguments must be given?\n",
        "\n",
        "    In scikit-learn, the model.predict() method is used to make predictions on new, unseen data after a model has been trained using the fit() method. It applies the learned patterns to input data and returns the predicted outcomes.\n",
        "\n",
        "\n",
        "        from sklearn.datasets import load_iris\n",
        "        from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "        # Load dataset\n",
        "        iris = load_iris()\n",
        "        X, y = iris.data, iris.target\n",
        "\n",
        "        # Train model\n",
        "        model = KNeighborsClassifier()\n",
        "        model.fit(X, y)\n",
        "\n",
        "        # Predict class labels for new data\n",
        "        new_data = [[5.1, 3.5, 1.4, 0.2], [6.0, 3.0, 4.7, 1.5]]\n",
        "        predictions = model.predict(new_data)\n",
        "        print(predictions)\n"
      ],
      "metadata": {
        "id": "nYRL6KiKs57W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What are continuous and categorical variables?\n",
        "\n",
        "      Continuous variables: Numeric, infinite precision, allow full mathematical operations.\n",
        "\n",
        "      Categorical variables: Grouped into categories (nominal or ordinal), no numeric meaning between values"
      ],
      "metadata": {
        "id": "Z47VFKCfrt0U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        "    Feature scaling is a crucial data preprocessing technique in machine learning that normalizes the range of feature values so they contribute fairly during model training.\n",
        "\n",
        "    Feature scaling transforms numeric features to a similar scale using techniques such as\n",
        "\n",
        "    1. Standardization\n",
        "    2. Normalization\n",
        "    3. Unit vector\n"
      ],
      "metadata": {
        "id": "n4foC7mqtUGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. How do we perform scaling in Python?\n",
        "\n",
        "          from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "          from sklearn.model_selection import train_test_split\n",
        "          import numpy as np, pandas as pd\n",
        "\n",
        "                 from sklearn.model_selection import train_test_split\n",
        "          import numpy as np, pandas as pd\n",
        "\n",
        "\n",
        "           data = {'height': [170,180,165,175], 'weight': [70,80,65,75]}\n",
        "          df = pd.DataFrame(data)\n",
        "          X = df.values\n",
        "\n",
        "          X_train, X_test = train_test_split(X, test_size=0.5, random_state=1)\n",
        "\n",
        "          scaler = MinMaxScaler()\n",
        "          X_train_scaled = scaler.fit_transform(X_train)\n",
        "          X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "          print(\"Train scaled:\\n\", X_train_scaled)\n",
        "          print(\"Test scaled:\\n\", X_test_scaled)\n"
      ],
      "metadata": {
        "id": "psw3PUF7uQR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is sklearn.preprocessing?\n",
        "\n",
        "      sklearn.preprocessing is a powerful module in scikit-learn that provides a wide variety of transformers to preprocess your data before training a machine learning model."
      ],
      "metadata": {
        "id": "jb-LMmiYupTG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How do we split data for model fitting (training and testing) in Python?\n",
        "    1. Import the function\n",
        "    2. Decide your data arrays\n",
        "    3. Use train_test_split\n",
        "\n",
        "\n",
        "    import pandas as pd\n",
        "      from sklearn.model_selection import train_test_split\n",
        "\n",
        "      df = pd.read_csv('data.csv')\n",
        "      X = df.drop('target', axis=1)\n",
        "      y = df['target']\n",
        "\n",
        "      X_train, X_test, y_train, y_test = train_test_split(\n",
        "          X, y,\n",
        "          test_size=0.25,\n",
        "          random_state=104,\n",
        "          shuffle=True\n",
        "      )\n",
        "\n",
        "      print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "52-5iTiWvDVk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Explain data encoding?\n",
        "\n",
        "      Data encoding refers to the process of converting categorical data (like strings or text labels) into numeric values that machine learning models can process.\n",
        "\n",
        "      Types:\n",
        "      1. Nominal\n",
        "      2. Label & ordinal\n",
        "      3. Target Guided Ordinal Encoding"
      ],
      "metadata": {
        "id": "ppVktvtGvYFR"
      }
    }
  ]
}